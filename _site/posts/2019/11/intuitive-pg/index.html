

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Intuitive Explanation of Policy Gradient - Arun Kumar</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Arun Kumar">
<meta property="og:title" content="Intuitive Explanation of Policy Gradient">


  <link rel="canonical" href="http://localhost:4000/posts/2019/11/intuitive-pg/">
  <meta property="og:url" content="http://localhost:4000/posts/2019/11/intuitive-pg/">



  <meta property="og:description" content="In this post I will give an intuitive explaination of Policy Gradient so that it becomes easier to implement. Much of what is known to a developer about the Policy Gradient method in Reinforcement Learning is still not very clear given the kind of language used in the research papers and the blog articles.">



  <meta name="twitter:site" content="@ioarun">
  <meta name="twitter:title" content="Intuitive Explanation of Policy Gradient">
  <meta name="twitter:description" content="In this post I will give an intuitive explaination of Policy Gradient so that it becomes easier to implement. Much of what is known to a developer about the Policy Gradient method in Reinforcement Learning is still not very clear given the kind of language used in the research papers and the blog articles.">
  <meta name="twitter:url" content="http://localhost:4000/posts/2019/11/intuitive-pg/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="http://localhost:4000/images/site-logo.png">
    
  

  
    <meta name="twitter:creator" content="@arun">
  



  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2019-11-05T13:56:19-08:00">











<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Kumar Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">

<script type="application/ld+json"> 
{ 
"@context": "http://schema.org",
"@type": "Person",
"name": "R. Stuart Geiger",
"email": "mailto:stuart@stuartgeiger.com",
"image": "http://stuartgeiger.com/images/oban3.jpg",
"jobTitle": "Ethnographer",
"name": "R. Stuart Geiger",
"affiliation": "University of California, Berkeley",
"alumniOf": "University of California, Berkeley",
"birthPlace": "Nacogdoches County, TX",
"gender": "male",
"honorificSuffix": "PhD",
"nationality": "United States",
"url": "http://www.stuartgeiger.com",
"sameAs" : [
"http://twitter.com/staeiou",
"http://github.com/staeiou",
"https://orcid.org/0000-0001-7215-0532"] 
} </script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Arun Kumar</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/expressions/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/fun/">Fun</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/contact/">Contact</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    

    <script type="application/ld+json">
    {
	"@context": "http://schema.org/",
	"@type": "CreativeWork",
	"author": "Arun Kumar",
        "name": "Intuitive Explanation of Policy Gradient",
	"datePublished": "2019-11-05 13:56:19 -0800",
	"description": "In this post I will give an intuitive explaination of Policy Gradient so that it becomes easier to implement. Much of what is known to a developer about the Policy Gradient method in Reinforcement Learning is still not very clear given the kind of language used in the research papers and the blog articles."
    }
    </script>




<div id="main" role="main">
  



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Intuitive Explanation of Policy Gradient">
    <meta itemprop="description" content="In this post I will give an intuitive explaination of Policy Gradient so that it becomes easier to implement. Much of what is known to a developer about the Policy Gradient method in Reinforcement Learning is still not very clear given the kind of language used in the research papers and the blog articles.">
    <meta itemprop="datePublished" content="November 05, 2019">
    
    


    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Intuitive Explanation of Policy Gradient
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
          
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-11-05T13:56:19-08:00">November 05, 2019</time></p>
        
        </header>
      

      <section class="page__content" itemprop="text">
        <html>
<head>
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<head>
<body>

In this post I will give an intuitive explaination of Policy Gradient so that it becomes easier to implement. Much of what is known to a developer about the Policy Gradient method in Reinforcement Learning is still not very clear given the kind of mathematical language used in the research papers and the blog articles. It might be confusing as to how the algorithm is applied to discrete and continuous actions with very similar structure of the policy gradient. The reason for writing this article is to put down my thoughts and understanding of the topic and hopefully give a helpful insights to the reader. So let's get started.<br /><br />
<!--more-->

To summarize Policy Gradient in one line, it can be said that it is <b>the method by which we try to maximize the likelihood of the sequence of actions that cause favourable outcomes</b>. These favourable outcomes can be winning a game of DOTA, landing a rover on moon, achieving walking gait by a robot, etc. And how do we maximize the likelihood of favourable outcomes ? We will come to that later but first let's start from the begining. The policy in policy gradient is represented (mostly) by a function approximator like a neural network. How do we model a neural network? We model it using network parameters `\theta` that is nothing but network weights. We can call this policy network as `\pi_theta` (policy π parameterized with `\theta`). The input to this network (policy) is the current state (`s_t`) and the output is a <b>probability distribution</b> over next action.

<h2> Policy Gradient for Discrete Actions </h2>

<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2019/pg/policy-network.png" title="policy">
    
        <img align="middle" src="http://localhost:4000/images/2019/pg/policy-network.png" alt="policy" style="border: 2px solid black; width: 500px; height: 400px" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:12px;">A neural network policy. Being stochastic in nature, it doesn't predict the exact actions to take but the probability distribution over possible actions.</p>
    
</div>
</center>


Let's take an example of Pong. There are two discrete actions - UP and DOWN. At any instant, we have to chose either of the two actions. There can never be a scenario where we have to chose both the actions simultaneously (a case where we use joint probability distribution is discussed later).<br /><br />

So, the agent observes the current state and the policy network outputs a probability over two possible actions : p(UP|state) and p(DOWN|state) The output can be modelled as a softmax function where the sum of the outputs equals 1. The next step would be to <b>sample</b> from this output. This makes our policy a stochastic one. The way it is usually accomplished is something like 
<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="p">])</span> </code></pre></figure>
 where the first argument is the choice of actions we have (0=UP, 1=DOWN), second argument is their corresponding probabilities predicted as explained before. 
<br /><br />

Let's say probability of UP is 0.6 and probability of DOWN is 0.4. During sampling, it's possible to have sampled the action DOWN (ofcourse with lesser probability of occurance). In the next step, this action is executed and the next state and reward is observed. The process repeats till the end of the episode and we maintain a history of the transitionss [current_state, action, next_state, reward] through this episode. At the end of it, the discounted sum of the rewards is calculated and we'll use it for updating our policy.<br /><br />

The update to the network happens by backpropagating the <b>gradient of the product of the log probability of the trajectory and the discounted sum of rewards</b>.

$$
\nabla_{\theta}\log{P(\tau^{(i)};\theta)R(\tau^{(i)})}
$$

In actual implementation, we gather `m` such episodes (trajectories) and find average of the gradient of the above form and backpropagate the result. Let's dive into the term `\log{P(\tau^{(i)})}` from the above equation. This term is known as <b>log-likelihood</b>. It measures the likelihood of the observed trajectory or the sequence of actions. The likelihood, in other words is nothing but probability distribution. <b>If this term is high, the corresponding sequence of actions are more likely to occur or to be observed.</b> Our goal is to update our policy network weights in such a way that the log-likelihood of favourable sequences/trajectories/episodes increase and that of the unfavourable ones decrease. Further simplifying the expression as follows we have :
$$
\nabla_{\theta}\log{P(\tau^{(i)};\theta)} = \nabla_{\theta}\log{[\prod_{t=0}^{H}P(s_{t+1}^{(i)}|s_{t}^{(i)}, u_{t}^{(i)}).\pi_\theta(u_{t}^{(i)}|s_{t}^{(i)})]}
$$

Note that individual trajectory `\tau^{i}` is composed of a state-action sequence `s_0, u_0,..., s_H, u_H`. From the multiplication rule in probability it can be said that the probability of a trajectory is the product of the probabilities of each transition that occurred from initial state `s_0` to be the terminal state `s_H`.Now how do we calculate probability of each transition? The probability of a transition is the product of the probability of ending up in the next state `s_{t+1}` from current state `s_t`(dynamics model) and the probability of taking an action `u_t` given current state `s_t` under the policy `\pi_\theta`. We can rewrite the above expression as follows:

$$
= \nabla_\theta[\sum_{t=0}^{H}\log{P(s_{t+1}^{(i)}|s_{t}^{(i)}, u_{t}^{(i)}) + \sum_{t=0}^{H}\log{\pi_\theta(u_{t}^{(i)}|s_{t}^{(i)})}}]
$$

As the first term doesn't depend on `\theta`, the above expression boils down to:

$$
= \sum_{t=0}^{H}\log{\pi_\theta(u_{t}^{(i)}|s_{t}^{(i)})}
$$

So the final expression doesn't depend on the dynamics model. Hence applying policy gradient is so convenient.

<h3>Log-likelihood for discrete actions</h3>

The term `\log\pi_\theta(u_{t}^{(i)}|s_{t}^{(i)})` is read as the the log of the probability of the action chosen (executed) given the current state. And we implement it as follows:


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">prob</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="c"># 1-prob = 0.4</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c"># if action_selected = 1</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="p">)</span> <span class="c"># fake labels : 0, 1</span>

<span class="c"># if action_selected = 0</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="p">)</span> <span class="c"># fake labels : 1, 0</span></code></pre></figure>


<h2> Policy Gradient for Continuous Actions </h2>

<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2019/pg/policy-network-continuous.png" title="policy">
    
        <img align="middle" src="http://localhost:4000/images/2019/pg/policy-network-continuous.png" alt="policy" style="border: 2px solid black; width: 500px; height: 400px" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:12px;">A neural network policy that outputs a probability distribution over actions.</p>
    
</div>
</center>


The policy network for the continuous action space is similar to the discrete actions except that the probability distribution is not discrete but continuous and is defined by the parameters `\mu` and `\sigma` . These parameters are the output of the network. In case where there are multiple outputs (multi-dimensional actions, or multivariate outputs), the joint probability distribution is found by multiplying individual probability density functions. In case where the output is a single action (e.g, one motor torque), the likelihood (or probability distribution) is very straightforward. At every time-step in the episode, we sample action `x` from this distribution whose mean is `\mu` and std. deviation is `\sigma`. The way the sampling is done may look as follows:


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_sigma</span><span class="p">))</span></code></pre></figure>


The likelihood of the sequence of actions or the probability distribution is represented as follows:

$$
= \frac{1}{2\pi\sigma^{2}}e^{-\frac{(X - \mu)^2}{2\sigma^{2}}}
$$

where X = the sequence of sampled action over one episode and `\mu` is the sequence of the predicted action (mean) by the policy network.

<h3>Log-likelihood for continuous actions</h3>
The log of the above expression is the log-likelihood and it is of the following form:


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">log_likelihood</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_stddev</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_stddev</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>



<h2>What if we have multi-dimensional actions?</h2>

There can be scenarios where there are multiple actions occuring simultaneously. We might be required to build an agent that handles multiple discrete actions (e.g, GAS = ON, TURN_LEFT = ON, TURN_RIGHT = OFF, GO_FORWARD = OFF) or multiple continuous actions (e.g, control a 6 DOF arm that has 6 motors to control simulataneously). Let's take these two cases one by one.

<h3>Multi-dimensional discrete actions</h3>
The log-likelihood expression of the case of two actions (UP &amp; DOWN as before) will change a bit.


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">prob_gas</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">prob_go_left</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">prob_go_right</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">prob_go_forward</span> <span class="o">=</span> <span class="mf">0.1</span>

 <span class="c"># fake labels: [1, 1, 0, 0] =&gt; GAS, LEFT, RIGHT, FORWARD</span>
<span class="c"># if action_selected = 1</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">1.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_gas</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_go_left</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_go_right</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_go_forward</span><span class="p">)</span></code></pre></figure>


If we go one step back, the expression of the log_likelihood as written above will become clear. The log of likelihood is expressed as the sum of the log. prob terms. If we remove log terms from both sides of the equation, the expression on the right hand side becomes the <b>product of probabilities of individual actions</b> hence, we can say it is sort of joint probability distribution over actions.

<h3>Multi-dimensional continuous actions</h3>

Before estimating the log-likelihood, we have to find the corresponding probability distribution and the way we do it is by finding joint probability distribution. We do this because the multi-dimensional actions are independent of each other for e.g, in the case of a 6 DOF arm control that has 6 motors to control simulataneously. The final joint probability distribution is a multi-variate probability distribution that is the resultant of the <b>product of individual probability distribution of each action</b>. 

$$
 = \frac{1}{(2\pi)^{(n/2)}|\sum^{1/2}|}\exp(-\frac{1}{2}(x - \mu)^T\sum^{-1}(x - \mu))
$$

Note that for single action, the above equation boils down to the single-dimensional continuous action space case as explained in previous sections. The log of the above equation gives the log-likelihood of form very similar to the one discussed as in the case of single-dimensional action.

<h2>Maximize the likelihood by updating weights</h2>

The next step from here is to maximize this log-likelihood expression. And the way this can be done is by using various optimization algorithm. One such algorithm is gradient ascent. For that, we find the derivative of our log-likelihood term with respect to the network parameters (weights) and backpropagate the term through the network. 


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">+</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">derivative_of_the_log_likelihood_wrt_weights</span><span class="p">)</span></code></pre></figure>
 

<h2>Acknowledgement</h2>
I'd like to thank Pradeep Rengaswamy (IIT Kharagpur) for contributing to the discussions that led to the content of this article.


</body>
<html>



</html></head></head></html>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#artificial-intelligence" class="page__taxonomy-item" rel="tag">artificial-intelligence</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#policy-gradient" class="page__taxonomy-item" rel="tag">policy-gradient</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#reinforcement-learning" class="page__taxonomy-item" rel="tag">reinforcement-learning</a>
    
    </span>
  </p>




  






  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/categories/#academic-works" class="page__taxonomy-item" rel="tag">Academic Works</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/categories/#unpublished" class="page__taxonomy-item" rel="tag">Unpublished</a>
    
    </span>
  </p>


      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/posts/2019/11/intuitive-pg/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/2019/11/intuitive-pg/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http://localhost:4000/posts/2019/11/intuitive-pg/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/posts/2019/11/intuitive-pg/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="http://localhost:4000/posts/2017/04/robotic-car/" class="pagination--pager" title="Build a 2D Robotic Car!
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
<!--   
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item">
    


    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2017/04/robotic-car/" rel="permalink">Build a 2D Robotic Car!
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
    
    
    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2017-04-03T14:56:19-07:00">April 03, 2017</time></p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>In the last blog post I explained what is a particle filter and how we can build one using pygame and python.In this post, I will walk you through the steps to build a 2D robotic car and get it running using PD control.
<u><strong><a href="http://localhost:4000/posts/2017/04/robotic-car/" rel="permalink"> Read more</a></strong></u></p></p>
    

    
    

    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item">
    


    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2017/03/particle-filter/" rel="permalink">Robot Localization using Particle Filter
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
    
    
    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2017-03-07T13:56:19-08:00">March 07, 2017</time></p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Robot world is exciting! For people completely unaware of what goes inside the robots and how they manage to do what they do, it seems almost magical.In this post, with the help of an implementation, I will try to scratch the surface of one very important part of robotics called robot localization.
<u><strong><a href="http://localhost:4000/posts/2017/03/particle-filter/" rel="permalink"> Read more</a></strong></u></p></p>
    

    
    

    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item">
    


    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2017/01/cnn/" rel="permalink">Convolutional Neural Network
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
    
    
    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2017-01-30T13:56:19-08:00">January 30, 2017</time></p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Convolutional Neural Networks or ConvNets or CNNs are biologically inspired varients of Multilayer Perceptrons(MLPs).They are probably the biggest reasons why AI agents are able to play ATARI games, are creating master piece artwork and cars have learnt to drive by themselves.Not only this, they are also being used in Natural language processing and text classification.
<u><strong><a href="http://localhost:4000/posts/2017/01/cnn/" rel="permalink"> Read more</a></strong></u></p></p>
    

    
    

    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item">
    


    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2017/01/htm/" rel="permalink">Understanding Hierarchical Temporal Memory
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  8 minute read
	
</p>
    
    
    
      <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2017-01-15T13:56:19-08:00">January 15, 2017</time></p>
    

    
    <p class="archive__item-excerpt" itemprop="description"><p>Last year I did one project on Cognitive Healthcare which used Hierarchical Temporal Memory or HTM.Through this post, I have tried to put down my understanding of <a href="http://numenta.org/">Numenta</a>’s HTM. Before getting to it, it is important to understand the functioning of the neocortex to process sensory inputs from the physical world because HTM is inspired by the same.
<u><strong><a href="http://localhost:4000/posts/2017/01/htm/" rel="permalink"> Read more</a></strong></u></p></p>
    

    
    

    

  </article>
</div>

        
      </div>
    </div>
   -->
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/ioarun"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="http://github.com/ioarun"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
   <!--  <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Arun Kumar. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>





  </body>
</html>

